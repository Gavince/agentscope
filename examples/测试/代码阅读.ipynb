{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "201fa4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentscope.agent import ReActAgent, UserAgent\n",
    "from agentscope.formatter import DashScopeChatFormatter\n",
    "from agentscope.memory import InMemoryMemory\n",
    "from agentscope.model import DashScopeChatModel\n",
    "from agentscope.tool import (\n",
    "    Toolkit,\n",
    "    execute_shell_command,\n",
    "    execute_python_code,\n",
    "    view_text_file,\n",
    ")\n",
    "\n",
    "from agentscope.message import (\n",
    "    Msg,\n",
    "    ToolUseBlock,\n",
    "    ToolResultBlock,\n",
    "    TextBlock,\n",
    "    AudioBlock,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a13da37",
   "metadata": {},
   "source": [
    "## 记忆模块\n",
    "InMemoryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dde690c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<agentscope.memory._in_memory_memory.InMemoryMemory object at 0x10d5b8aa0>\n"
     ]
    }
   ],
   "source": [
    "print(InMemoryMemory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb999815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Msg(id='Q3ZwnPJHDufMpJhbSfwHYb', name='user', content='Hello, how are you?', role='user', metadata=None, timestamp='2026-01-09 08:02:56.901', invocation_id='None'), Msg(id='SdZhZxCVeQs5D6XSJM2cPb', name='assistant', content=\"I'm fine, thank you!\", role='assistant', metadata=None, timestamp='2026-01-09 08:02:56.901', invocation_id='None')]\n"
     ]
    }
   ],
   "source": [
    "memory = InMemoryMemory()\n",
    "await memory.add(Msg(role=\"user\", content=\"Hello, how are you?\", name=\"user\"))\n",
    "await memory.add(Msg(role=\"assistant\", content=\"I'm fine, thank you!\", name=\"assistant\"))\n",
    "print(await memory.get_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b677a8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一条短消息\n",
    "message = Msg(role=\"user\", content=\"Hello, how are you?\", name=\"user\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d7cf6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Msg(id='WY4GJpiKUZsLUfR7VornoM', name='user', content='Hello, how are you?', role='user', metadata=None, timestamp='2026-01-09 07:49:47.588', invocation_id='None')\n"
     ]
    }
   ],
   "source": [
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62dc3a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(memory.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3af19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[Msg(id='aEVxX9aLJw84japJB9sweY', name='1', content='你好', role='user', metadata=None, timestamp='2026-01-09 08:08:50.902', invocation_id='None'), Msg(id='iK8QcowHEaxKSvcrcu4wd6', name='2', content='こんにちは！', role='assistant', metadata=None, timestamp='2026-01-09 08:08:50.902', invocation_id='None')]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "memory = InMemoryMemory()\n",
    "\n",
    "await memory.add(Msg(name=\"1\", role=\"user\", content=\"你好\"))\n",
    "await memory.add(Msg(name=\"2\", role=\"assistant\", content=\"こんにちは！\"))\n",
    "\n",
    "print(await memory.size())          # 输出: 2\n",
    "print(await memory.get_memory())    # 输出: [Msg1, Msg2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d78f40af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': [{'id': 'aEVxX9aLJw84japJB9sweY', 'name': '1', 'role': 'user', 'content': '你好', 'metadata': None, 'timestamp': '2026-01-09 08:08:50.902'}, {'id': 'iK8QcowHEaxKSvcrcu4wd6', 'name': '2', 'role': 'assistant', 'content': 'こんにちは！', 'metadata': None, 'timestamp': '2026-01-09 08:08:50.902'}]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 保存状态\n",
    "state = memory.state_dict()\n",
    "print(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ee71c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 清空后恢复\n",
    "await memory.clear()\n",
    "memory.load_state_dict(state)\n",
    "print(await memory.size())          # 再次输出: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b0ec02",
   "metadata": {},
   "source": [
    "## 模型调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "243d6742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 非流式调用演示 ===\n",
      "\n",
      "用户：北京2026年天气怎么样？\n",
      "\n",
      "Friday（一次性完整回复）：\n",
      "目前无法提供2026年北京具体的天气预报。天气预报通常只能准确预测未来几天到一周左右的情况，而长期的气候预测则依赖于气候模型和历史数据，但这些预测也主要是趋势性的，并不能精确到具体某一天的天气状况。\n",
      "\n",
      "如果您对北京未来几年的大致气候变化感兴趣，可以关注气象科研机构发布的气候研究报告或长期气候预测。这类信息可能会给出一些关于温度、降水等平均值变化的趋势性描述，但依然存在不确定性。对于更短期的天气情况，请参考当地气象部门发布的最新天气预报。\n",
      "\n",
      "\n",
      "Token 使用：输入 30 + 输出 119 = 总计 149\n",
      "\n",
      "\n",
      "=== 流式调用演示 ===\n",
      "\n",
      "用户：北京2026年天气怎么样？\n",
      "\n",
      "Friday（实时打字）：目前无法提供2026年的具体天气预报，因为长期天气预测技术尚不能准确到数年后的具体天气情况。通常，可靠的天气预报可以提前大约一周给出，而气候趋势的预测则可以在更长的时间范围内（如几个月或一个季节）进行一定程度上的估计。\n",
      "\n",
      "如果您对北京未来几年可能面临的气候变化感兴趣，可以关注气象科学研究机构发布的关于全球及区域气候变化的趋势报告。这些报告虽然不能提供具体的天气状况，但能够帮助人们了解未来气候的大致走向。例如，根据一些研究，全球变暖可能会导致某些地区的极端天气事件变得更加频繁和强烈。不过，请注意实际天气情况会受到多种因素的影响，具体情况还需以官方气象部门发布的最新信息为准。\n",
      "\n",
      "（流式输出结束）\n",
      "\n",
      "\n",
      "=== 总结对比 ===\n",
      "• 非流式（stream=False）：\n",
      "  - 优点：简单，一次性拿到完整结果，便于后续处理（如总结、存储）\n",
      "  - 缺点：用户需要等待整个回复生成完毕，体验上有延迟\n",
      "  - 适合：后台任务、批量生成、需要完整文本分析的场景\n",
      "\n",
      "• 流式（stream=True + incremental_output=True）：\n",
      "  - 优点：实时显示“打字”效果，用户感知响应更快，体验更好\n",
      "  - 缺点：代码稍复杂，需要 async for 处理\n",
      "  - 适合：聊天界面、WebSocket、实时交互应用\n",
      "\n",
      "常用参数建议：\n",
      "• temperature：0.3~0.5（更确定、严谨） vs 0.7~0.9（更有创意）\n",
      "• top_p：通常设 0.8~0.95，与 temperature 互补\n",
      "• max_tokens：根据上下文窗口控制，避免超限（qwen-max 支持最大 ~32k tokens）\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from dashscope.aigc.generation import AioGeneration\n",
    "\n",
    "# 请提前设置环境变量：export DASHSCOPE_API_KEY=sk-xxxxxxxxxxxx\n",
    "# 或者直接在这里填写（不推荐）\n",
    "DASHSCOPE_API_KEY = os.getenv(\"DASHSCOPE_API_KEY\", \"your-api-key-here\")\n",
    "\n",
    "async def non_stream_demo():\n",
    "    \"\"\"非流式调用：一次性返回完整结果（适合后台处理、需要完整文本的场景）\"\"\"\n",
    "    print(\"\\n=== 非流式调用演示 ===\\n\")\n",
    "    print(\"用户：北京2026年天气怎么样？\\n\")\n",
    "    print(\"Friday（一次性完整回复）：\")\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"qwen-max\",                  # 可换成 qwen-plus、qwen-turbo 等\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant named Friday.\"},\n",
    "            {\"role\": \"user\", \"content\": \"北京2026年天气怎么样？\"}\n",
    "        ],\n",
    "        \"stream\": False,                      # 关键：关闭流式\n",
    "        \"result_format\": \"message\",\n",
    "        \"temperature\": 0.7,                   # 控制创造性：0.0~1.0，越高越随机\n",
    "        \"top_p\": 0.8,                         # 核采样，建议和 temperature 二选一调整\n",
    "        \"max_tokens\": 512,                    # 最大输出 token 数，防止太长\n",
    "    }\n",
    "\n",
    "    response = await AioGeneration.call(\n",
    "        api_key=DASHSCOPE_API_KEY,\n",
    "        **payload\n",
    "    )\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        content = response.output.choices[0].message.content\n",
    "        print(content)\n",
    "\n",
    "        # 显示 token 使用情况\n",
    "        usage = response.usage\n",
    "        print(f\"\\n\\nToken 使用：输入 {usage.input_tokens} + 输出 {usage.output_tokens} = 总计 {usage.total_tokens}\")\n",
    "    else:\n",
    "        print(f\"请求失败：{response.message}\")\n",
    "\n",
    "async def stream_demo():\n",
    "    \"\"\"流式调用：实时逐字/逐 token 返回（适合聊天界面“打字机”效果）\"\"\"\n",
    "    print(\"\\n\\n=== 流式调用演示 ===\\n\")\n",
    "    print(\"用户：北京2026年天气怎么样？\\n\")\n",
    "    print(\"Friday（实时打字）：\", end=\"\", flush=True)\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"qwen-max\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant named Friday.\"},\n",
    "            {\"role\": \"user\", \"content\": \"北京2026年天气怎么样？\"}\n",
    "        ],\n",
    "        \"stream\": True,                       # 关键：开启流式\n",
    "        \"incremental_output\": True,           # 推荐：只返回增量内容（更自然的打字效果）\n",
    "        \"result_format\": \"message\",\n",
    "        \"temperature\": 0.85,                  # 流式时可以稍微提高，回复更生动\n",
    "        \"max_tokens\": 512,\n",
    "    }\n",
    "\n",
    "    response = await AioGeneration.call(\n",
    "        api_key=DASHSCOPE_API_KEY,\n",
    "        **payload\n",
    "    )\n",
    "\n",
    "    full_content = \"\"\n",
    "    async for chunk in response:\n",
    "        if chunk.status_code != 200:\n",
    "            print(f\"\\n\\n流式错误：{chunk.message}\")\n",
    "            return\n",
    "\n",
    "        if chunk.output and chunk.output.choices:\n",
    "            delta = chunk.output.choices[0].message.content\n",
    "            if delta:\n",
    "                print(delta, end=\"\", flush=True)\n",
    "                full_content += delta\n",
    "\n",
    "    print(\"\\n\\n（流式输出结束）\")\n",
    "    # 流式结束后，chunk 中也会带最终的 usage\n",
    "    if hasattr(response, \"usage\"):\n",
    "        usage = response.usage\n",
    "        print(f\"Token 使用：输入 {usage.input_tokens} + 输出 {usage.output_tokens} = 总计 {usage.total_tokens}\")\n",
    "\n",
    "async def main():\n",
    "    # 依次运行两个 demo，让你直观对比区别\n",
    "    await non_stream_demo()\n",
    "    await stream_demo()\n",
    "\n",
    "    print(\"\\n\\n=== 总结对比 ===\")\n",
    "    print(\"• 非流式（stream=False）：\")\n",
    "    print(\"  - 优点：简单，一次性拿到完整结果，便于后续处理（如总结、存储）\")\n",
    "    print(\"  - 缺点：用户需要等待整个回复生成完毕，体验上有延迟\")\n",
    "    print(\"  - 适合：后台任务、批量生成、需要完整文本分析的场景\")\n",
    "    print(\"\")\n",
    "    print(\"• 流式（stream=True + incremental_output=True）：\")\n",
    "    print(\"  - 优点：实时显示“打字”效果，用户感知响应更快，体验更好\")\n",
    "    print(\"  - 缺点：代码稍复杂，需要 async for 处理\")\n",
    "    print(\"  - 适合：聊天界面、WebSocket、实时交互应用\")\n",
    "    print(\"\")\n",
    "    print(\"常用参数建议：\")\n",
    "    print(\"• temperature：0.3~0.5（更确定、严谨） vs 0.7~0.9（更有创意）\")\n",
    "    print(\"• top_p：通常设 0.8~0.95，与 temperature 互补\")\n",
    "    print(\"• max_tokens：根据上下文窗口控制，避免超限（qwen-max 支持最大 ~32k tokens）\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5beffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edbf54b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
